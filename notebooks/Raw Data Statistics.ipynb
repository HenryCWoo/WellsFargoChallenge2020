{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Statistics\n",
    "A look into the raw distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "TRAINSET = '../data/raw/0173eeb640e7-Challenge+Data+Set+-+Campus+Analytics+2020.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and visually inspect/verify\n",
    "df = pd.read_excel(TRAINSET)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = df.drop(['XC', 'y'], axis=1)\n",
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean:', feat_df.stack().mean(), 'Std:', feat_df.stack().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, it appears that the entire data set follows some form of data distribution with a mean ~0 and std ~1. Therefore, it is not necessary to apply a normalization transform when preprocessing the data during the training and test phase.\n",
    "\n",
    "Below is a visual histogram of each column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_df_0 = df.loc[df['y'] == 0].drop(['XC', 'y'], axis=1)\n",
    "feat_df_1 = df.loc[df['y'] == 1].drop(['XC', 'y'], axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(30, 1, figsize=(8, 30*8))\n",
    "for i, col in enumerate(feat_df.columns):\n",
    "    feat_df_0.hist(column=col, ax=ax[i], bins=100, color='blue', alpha=0.5, label='Target 0')\n",
    "    feat_df_1.hist(column=col, ax=ax[i], bins=100, color='orange', alpha=0.5, label='Target 1')\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualizations above, we can see that there are clearly more data samples labelled with target 0 than target 1. \n",
    "Also, these histograms are plotted with the raw data values (no normalization) and appear to follow some form of unimodal normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the demonstration below, the data is assumed, for now, to follow a Gaussian distribution and the probability density functions of each target label for each feature are graphed on top of each other. This is to see if features from different target labels follow any sort of different distribution (mean or std deviation), even if the differences are small. The graphs above do not highlight or make this observation very perceptable because histograms are mere binnings of data value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(30, 1, figsize=(8, 30*8))\n",
    "\n",
    "x_axis = np.arange(-5, 5, 0.001)\n",
    "for i, col in enumerate(feat_df.columns):\n",
    "    mean_0, std_0 = feat_df_0[col].mean(), feat_df_0[col].std()\n",
    "    mean_1, std_1 = feat_df_1[col].mean(), feat_df_1[col].std()\n",
    "    ax[i].plot(x_axis, norm.pdf(x_axis, mean_0, std_0), label='Target_0')\n",
    "    ax[i].plot(x_axis, norm.pdf(x_axis, mean_1, std_1), label='Target_1')\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualizations above, it appears that the distributions of features between target labels are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'XC' Column\n",
    "A consideration of the 'XC' char-valued column.\n",
    "\n",
    "The counts and ratio of the binary split between target labels are demonstrated per value from the 'XC' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char values in column 'XC' for each target label 0 and 1\n",
    "cr_tab = pd.crosstab(df.XC, df.y)\n",
    "cr_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values by row\n",
    "cr_tab.div(cr_tab.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the class imbalance many examples will be __biased__ towards target label 0 due to the larger number of data samples that target label 0 has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation with Target Label\n",
    "Summarize variables and their importance in classifying the output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First change column 'XC' into mapped integers instead of leaving them as char\n",
    "from pandas.api.types import CategoricalDtype\n",
    "feat_df = df.drop(['y'], axis=1)\n",
    "cat_type = CategoricalDtype(\n",
    "            categories=['A', 'B', 'C', 'D', 'E'], ordered=True)\n",
    "feat_df.XC = feat_df.XC.astype(cat_type).cat.codes\n",
    "\n",
    "# Pairwise Pearson Correlation\n",
    "pw_pearson = feat_df.corrwith(df['y'], method='pearson')\n",
    "print(pw_pearson)\n",
    "pw_pearson.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairwise Pearson Correlation results above, we can see that many of the features have almost no __linear__ correlation to the output label 'y'. There are notable features that do in fact see some correlation as shown by the larger bars in the bar chart above.\n",
    "\n",
    "The categorical char colunn 'XC' has the greatest correlation with the target label, so it is best not to omit it from training.\n",
    "\n",
    "We will likely need to use non-linear classification methods such as neural nets. Furthermore, given that the dataset is magnitudes smaller than most datasets used to train neural networks, the networks trained should not be too deep as that would likely lead to overfitting. More regularization methods will also be included such as dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
